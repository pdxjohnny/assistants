# if you use anthropic llm
anthropic_api_key = "..."

# if you use openai - does not really make sense but possible!
openai_api_key = "..."

# if you use your own llm deployed, for example with anyscale:
model_url = "http://ollama:11434/v1/chat/completions"
# or use ollama, check docker compose too!
# model_url = "http://ollama:11434/v1/chat/completions"
# docker compose --profile api --profile ollama -f docker/docker-compose.yml up
# or use your own llm
# model_url = "http://localhost:8000/chat/completions"

# if your own llm needs an api key (authorization bearer token), for example with anyscale:
# model_api_key = "get it here https://app.endpoints.anyscale.com/credentials"

database_url = "postgres://postgres:secret@localhost:5432/mydatabase"
redis_url = "redis://redis/"
s3_endpoint = "http://minio:9000"
s3_access_key = "minioadmin"
s3_secret_key = "minioadmin"
s3_bucket_name = "mybucket"
